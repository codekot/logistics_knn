{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification. Linear models and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you need to implement Logistic Regression with l2 regularization using gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression loss:\n",
    "$$ L(w) = \\dfrac{1}{N}\\sum_{i=1}^N \\log(1 + e^{-\\langle w, x_i \\rangle y_i}) + \\frac{1}{2C} \\lVert w \\rVert^2  \\to \\min_w$$\n",
    "$$\\langle w, x_i \\rangle = \\sum_{j=1}^n w_{j}x_{ij} + w_{0},$$ $$ y_{i} \\in \\{-1, 1\\}$$ where $n$ is the number of features and $N$ is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent step:\n",
    "$$w^{(t+1)} := w^{(t)} + \\dfrac{\\eta}{N}\\sum_{i=1}^N y_ix_i \\Big(1 - \\dfrac{1}{1 + exp(-\\langle w^{(t)}, x_i \\rangle y_i)}\\Big) - \\eta \\frac{1}{C} w,$$\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2 points)** Implement the algorithm and use it to classify the digits (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) into \"even\" and \"odd\" categories. \"Even\" and \"Odd\" classes  should correspond to {-1, 1} labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopping criteria: either the number of iterations exceeds *max_iter* or $||w^{(t+1)} - w^{(t)}||_2 < tol$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "\n",
    "# #suppress warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    _estimator_type = \"classifier\"\n",
    "    \n",
    "    def __init__(self, eta=0.001, max_iter=1000, C=1.0, tol=1e-5, random_state=42, zero_init=False):\n",
    "        \"\"\"Logistic Regression classifier.\n",
    "        \n",
    "        Args:\n",
    "            eta: float, default=0.001\n",
    "                Learning rate.\n",
    "            max_iter: int, default=1000\n",
    "                Maximum number of iterations taken for the solvers to converge.\n",
    "            C: float, default=1.0\n",
    "                Inverse of regularization strength; must be a positive float.\n",
    "                Smaller values specify stronger regularization.\n",
    "            tol: float, default=1e-5\n",
    "                Tolerance for stopping criteria.\n",
    "            random_state: int, default=42\n",
    "                Random state.\n",
    "            zero_init: bool, default=False\n",
    "                Zero weight initialization.\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.random_state = np.random.RandomState(seed=random_state)\n",
    "        self.zero_init = zero_init\n",
    "        self.X_ext = None\n",
    "        self.loss_hist = []\n",
    "         \n",
    "    def get_sigmoid(self, X, weights):\n",
    "        def sigm(z):\n",
    "            return 1 / (1 + exp(-z))\n",
    "        lin = X@weights\n",
    "        return np.fromiter(map(sigm, lin), dtype='float')\n",
    "    \n",
    "    def get_loss(self, x, weights, y):\n",
    "        \"\"\"Calculate the loss.\"\"\"\n",
    "#         X_ext = self.X_ext\n",
    "        X_ext = self.X_ext\n",
    "        return (1/X_ext.shape[0])*np.log(1+np.exp(-(weights@X_ext.T)*y_train)).sum()+\\\n",
    "                ((np.linalg.norm(weights)**2)/(2*self.C))/X.shape[1]\n",
    "        np.log(1+np.exp(-(weights_@X_ext.T)*y_train)).sum()\n",
    "#         f = (weights * X_ext).sum()\n",
    "#         L = [np.log(1+np.exp(-f*i)) for i in y]\n",
    "#         #print(L)\n",
    "#         return sum([np.log(1+np.exp(-f*i)) for i in y])\n",
    "#         def loss(self, h, y):\n",
    "#             return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "#         f = (weights * X_ext).sum()\n",
    "        \n",
    "        \n",
    "     \n",
    "    def fit(self, X, y):\n",
    "        from mpmath import mp\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Target vector.        \n",
    "        \"\"\"\n",
    "        X_ext = np.hstack([np.ones((X.shape[0], 1)), X]) # a constant feature is included to handle intercept\n",
    "        self.X_ext = X_ext\n",
    "        num_features = X_ext.shape[1]\n",
    "        if self.zero_init:\n",
    "            self.weights_ = np.zeros(num_features) \n",
    "        else:\n",
    "            weight_threshold = 1.0 / (2 * num_features)\n",
    "            self.weights_ = self.random_state.uniform(low=-weight_threshold,\n",
    "                                                      high=weight_threshold, size=num_features) # random weight initialization\n",
    "#(y@X_ext)*(1 - 1/(1+np.exp(-((weights_@X_ext.T)*y_train))).sum())\n",
    "        for i in range(self.max_iter): \n",
    "            delta = (1/X_ext.shape[0])*(y@X_ext)*\\\n",
    "                (1 - 1/(1+np.exp(-((self.weights_@X_ext.T)*y))).sum()) - ((self.eta/self.C)*self.weights_)\n",
    "            delta = - delta \n",
    "            self.weights_ -= self.eta * delta\n",
    "            self.loss_hist.append(self.get_loss(X, self.weights_, y))\n",
    "            if np.linalg.norm(delta) < self.tol:\n",
    "                print(\"break\")\n",
    "                break\n",
    "        \n",
    "        \n",
    "                \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict positive class probabilities.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing positive class probabilities.\n",
    "        \"\"\"\n",
    "        X_ext = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        if hasattr(self, 'weights_'):\n",
    "            return self.get_sigmoid(X_ext, self.weights_)\n",
    "        else: \n",
    "            raise NotFittedError(\"CustomLogisticRegression instance is not fitted yet\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing predicted class labels.\n",
    "        \"\"\"\n",
    "        y_hat = self.predict_proba(X)\n",
    "        return [1 if e>0.5 else -1 for e in y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 21 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "_, axes = plt.subplots(nrows=3, ncols=7, figsize=(10, 5))\n",
    "for ax, image, label in zip(axes.flatten(), X, y):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.gray_r if label % 2 else plt.cm.afmhot_r)\n",
    "    ax.set_title(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "#y_train = \"<your code>\"\n",
    "#y_test = \"<your code>\"\n",
    "y_train = (y_train % 2) * 2 - 1\n",
    "y_test = (y_test % 2) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.unique(y_train) == [-1, 1]).all()\n",
    "assert (np.unique(y_test) == [-1, 1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    disp = metrics.plot_confusion_matrix(clf, X_test, y_test, normalize='true')\n",
    "    disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics.accuracy_score(y_pred=clf.predict(X_train), y_true=y_train), \\\n",
    "           metrics.accuracy_score(y_pred=clf.predict(X_test), y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = CustomLogisticRegression(max_iter=1, zero_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(lr_clf.get_sigmoid(np.array([[0.5, 0, 1.0], [0.3, 1.3, 1.0]]), np.array([0.5, -0.5, 0.1])),\n",
    "                   np.array([0.58662, 0.40131]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.26086881e-06,  0.00000000e+00,  8.34782508e-05,  1.09495639e-03,\n",
       "        4.42434729e-04,  9.74608578e-04,  2.71443445e-03,  1.19513029e-03,\n",
       "        1.12695639e-04, -1.39130418e-06,  3.38086916e-04,  5.03652113e-04,\n",
       "       -8.73739025e-04,  7.23478173e-04,  2.00904324e-03,  8.45217289e-04,\n",
       "        5.14782546e-05,  6.95652090e-07, -2.29565190e-05, -1.44834765e-03,\n",
       "       -5.23826024e-04,  1.75026066e-03,  8.30608595e-04, -1.68347806e-04,\n",
       "       -1.04347813e-05,  0.00000000e+00, -4.43130381e-04, -1.14226073e-03,\n",
       "        1.97078237e-03,  2.70052141e-03,  1.00382597e-03, -3.40869524e-04,\n",
       "       -2.08695627e-06,  0.00000000e+00, -1.35582592e-03, -2.10226062e-03,\n",
       "       -8.90434675e-05,  7.42956432e-04,  8.41739029e-04, -1.63478241e-04,\n",
       "        0.00000000e+00, -1.04347813e-05, -1.06782596e-03, -4.07721690e-03,\n",
       "       -1.68556501e-03,  2.07999975e-04, -2.47652144e-04, -3.57565174e-04,\n",
       "       -2.64347794e-05, -9.04347717e-06, -1.88521716e-04, -2.22469538e-03,\n",
       "       -1.01773901e-03, -4.36869512e-04, -1.12069552e-03, -7.90956426e-04,\n",
       "       -3.54782566e-05, -6.95652090e-07,  5.35652109e-05,  1.27791289e-03,\n",
       "        3.61739087e-04, -1.47269547e-03, -1.07826074e-03, -7.40869476e-04,\n",
       "       -5.63478193e-05])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14792\\1441533731.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m assert np.allclose(lr_clf.weights_, np.array([ 3.1000e-06,  0.0000e+00,  4.1800e-05,  5.4770e-04,  2.2130e-04,\n\u001b[0m\u001b[0;32m      2\u001b[0m         \u001b[1;36m4.8750e-04\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m1.3577e-03\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m5.9780e-04\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m5.6400e-05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m7.0000e-07\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;36m1.6910e-04\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m2.5190e-04\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m4.3700e-04\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m3.6190e-04\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m1.0049e-03\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;36m4.2280e-04\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m2.5700e-05\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m3.0000e-07\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1.1500e-05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m7.2440e-04\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m        \u001b[1;33m-\u001b[0m\u001b[1;36m2.6200e-04\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m8.7540e-04\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m4.1540e-04\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m8.4200e-05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m5.2000e-06\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert np.allclose(lr_clf.weights_, np.array([ 3.1000e-06,  0.0000e+00,  4.1800e-05,  5.4770e-04,  2.2130e-04,\n",
    "        4.8750e-04,  1.3577e-03,  5.9780e-04,  5.6400e-05, -7.0000e-07,\n",
    "        1.6910e-04,  2.5190e-04, -4.3700e-04,  3.6190e-04,  1.0049e-03,\n",
    "        4.2280e-04,  2.5700e-05,  3.0000e-07, -1.1500e-05, -7.2440e-04,\n",
    "       -2.6200e-04,  8.7540e-04,  4.1540e-04, -8.4200e-05, -5.2000e-06,\n",
    "        0.0000e+00, -2.2160e-04, -5.7130e-04,  9.8570e-04,  1.3507e-03,\n",
    "        5.0210e-04, -1.7050e-04, -1.0000e-06,  0.0000e+00, -6.7810e-04,\n",
    "       -1.0515e-03, -4.4500e-05,  3.7160e-04,  4.2100e-04, -8.1800e-05,\n",
    "        0.0000e+00, -5.2000e-06, -5.3410e-04, -2.0393e-03, -8.4310e-04,\n",
    "        1.0400e-04, -1.2390e-04, -1.7880e-04, -1.3200e-05, -4.5000e-06,\n",
    "       -9.4300e-05, -1.1127e-03, -5.0900e-04, -2.1850e-04, -5.6050e-04,\n",
    "       -3.9560e-04, -1.7700e-05, -3.0000e-07,  2.6800e-05,  6.3920e-04,\n",
    "        1.8090e-04, -7.3660e-04, -5.3930e-04, -3.7060e-04, -2.8200e-05]), atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc, test_acc = fit_evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8406402226861517, 0.8694444444444445)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14792\\4215115562.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert min(train_acc, test_acc) > 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Visualize the loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7173898064667629,\n",
       " 0.67735705612967,\n",
       " 0.6403201474765501,\n",
       " 0.606233426728683,\n",
       " 0.5750157980170658,\n",
       " 0.5465562170538385,\n",
       " 0.5207203738665845,\n",
       " 0.4973577026616829,\n",
       " 0.47630801042734455,\n",
       " 0.45740725420072725,\n",
       " 0.4404922370462353,\n",
       " 0.4254041838148971,\n",
       " 0.4119912828540187,\n",
       " 0.4001103450242682,\n",
       " 0.38962775249124953,\n",
       " 0.3804198634617217,\n",
       " 0.3723730186232965,\n",
       " 0.3653832694848068,\n",
       " 0.3593559233631404,\n",
       " 0.3542049770531531,\n",
       " 0.3498524922391718,\n",
       " 0.3462279505315667,\n",
       " 0.34326761426637986,\n",
       " 0.340913910348649,\n",
       " 0.33911484789071483,\n",
       " 0.3378234756871224,\n",
       " 0.3369973822477329,\n",
       " 0.33659823883210704,\n",
       " 0.3365913844172585,\n",
       " 0.33694545057680675,\n",
       " 0.33763202369291945,\n",
       " 0.33862534164427743,\n",
       " 0.3399020220259254,\n",
       " 0.34144081899645545,\n",
       " 0.34322240596856074,\n",
       " 0.34522918152800175,\n",
       " 0.3474450961606457,\n",
       " 0.3498554975718735,\n",
       " 0.3524469925868663,\n",
       " 0.3552073238173699,\n",
       " 0.35812525946643037,\n",
       " 0.36119049481503335,\n",
       " 0.3643935640926037,\n",
       " 0.3677257615767774,\n",
       " 0.3711790708971734,\n",
       " 0.3747461016338061,\n",
       " 0.37842003240424127,\n",
       " 0.38219455972564065,\n",
       " 0.3860638520195119,\n",
       " 0.3900225081993257,\n",
       " 0.3940655203451485,\n",
       " 0.39818824002598563,\n",
       " 0.402386347880481,\n",
       " 0.4066558261107186,\n",
       " 0.4109929335828175,\n",
       " 0.41539418326239297,\n",
       " 0.41985632174334164,\n",
       " 0.42437631065523285,\n",
       " 0.42895130975831836,\n",
       " 0.4335786615561397,\n",
       " 0.43825587727426574,\n",
       " 0.44298062407011113,\n",
       " 0.44775071335332134,\n",
       " 0.452564090109082,\n",
       " 0.4574188231281227,\n",
       " 0.4623130960573007,\n",
       " 0.46724519919362795,\n",
       " 0.47221352195257243,\n",
       " 0.47721654594854895,\n",
       " 0.482252838631811,\n",
       " 0.48732104743155935,\n",
       " 0.4924198943600767,\n",
       " 0.4975481710371493,\n",
       " 0.5027047340980133,\n",
       " 0.5078885009516104,\n",
       " 0.5130984458591198,\n",
       " 0.5183335963055714,\n",
       " 0.523593029639905,\n",
       " 0.5288758699611177,\n",
       " 0.5341812852302096,\n",
       " 0.5395084845894791,\n",
       " 0.5448567158723896,\n",
       " 0.5502252632887265,\n",
       " 0.5556134452711252,\n",
       " 0.5610206124702628,\n",
       " 0.5664461458871218,\n",
       " 0.571889455131733,\n",
       " 0.5773499767987049,\n",
       " 0.5828271729506775,\n",
       " 0.5883205297015784,\n",
       " 0.5938295558922371,\n",
       " 0.5993537818515289,\n",
       " 0.6048927582367841,\n",
       " 0.6104460549476984,\n",
       " 0.6160132601084548,\n",
       " 0.6215939791131837,\n",
       " 0.6271878337302795,\n",
       " 0.6327944612614406,\n",
       " 0.6384135137516249,\n",
       " 0.6440446572464066,\n",
       " 0.6496875710934918,\n",
       " 0.6553419472853952,\n",
       " 0.6610074898405078,\n",
       " 0.6666839142199954,\n",
       " 0.6723709467781545,\n",
       " 0.6780683242440303,\n",
       " 0.6837757932322623,\n",
       " 0.689493109781269,\n",
       " 0.6952200389170218,\n",
       " 0.7009563542407818,\n",
       " 0.7067018375392908,\n",
       " 0.7124562784160097,\n",
       " 0.7182194739421037,\n",
       " 0.723991228325957,\n",
       " 0.729771352600084,\n",
       " 0.7355596643243896,\n",
       " 0.7413559873047902,\n",
       " 0.7471601513262839,\n",
       " 0.7529719918996137,\n",
       " 0.7587913500207272,\n",
       " 0.764618071942286,\n",
       " 0.7704520089565308,\n",
       " 0.7762930171888487,\n",
       " 0.7821409574014356,\n",
       " 0.7879956948064839,\n",
       " 0.7938570988883589,\n",
       " 0.7997250432342673,\n",
       " 0.8055994053729479,\n",
       " 0.8114800666209424,\n",
       " 0.8173669119360402,\n",
       " 0.8232598297775016,\n",
       " 0.8291587119727034,\n",
       " 0.8350634535898626,\n",
       " 0.8409739528165152,\n",
       " 0.8468901108434516,\n",
       " 0.8528118317538242,\n",
       " 0.8587390224171558,\n",
       " 0.8646715923880055,\n",
       " 0.8706094538090452,\n",
       " 0.8765525213183316,\n",
       " 0.8825007119605593,\n",
       " 0.8884539451020956,\n",
       " 0.8944121423496139,\n",
       " 0.9003752274721422,\n",
       " 0.9063431263263684,\n",
       " 0.9123157667850362,\n",
       " 0.9182930786682884,\n",
       " 0.9242749936778129,\n",
       " 0.9302614453336626,\n",
       " 0.9362523689136184,\n",
       " 0.9422477013949783,\n",
       " 0.9482473813986596,\n",
       " 0.9542513491355084,\n",
       " 0.9602595463547111,\n",
       " 0.9662719162942179,\n",
       " 0.97228840363308,\n",
       " 0.9783089544456215,\n",
       " 0.9843335161573585,\n",
       " 0.9903620375025907,\n",
       " 0.9963944684835883,\n",
       " 1.0024307603313105,\n",
       " 1.0084708654675816,\n",
       " 1.0145147374686654,\n",
       " 1.0205623310301775,\n",
       " 1.0266136019332786,\n",
       " 1.0326685070120913,\n",
       " 1.0387270041222918,\n",
       " 1.0447890521108285,\n",
       " 1.050854610786714,\n",
       " 1.0569236408928542,\n",
       " 1.062996104078864,\n",
       " 1.0690719628748366,\n",
       " 1.0751511806660212,\n",
       " 1.0812337216683758,\n",
       " 1.087319550904959,\n",
       " 1.0934086341831266,\n",
       " 1.0995009380725036,\n",
       " 1.105596429883695,\n",
       " 1.1116950776477155,\n",
       " 1.1177968500961009,\n",
       " 1.123901716641679,\n",
       " 1.1300096473599754,\n",
       " 1.1361206129712296,\n",
       " 1.1422345848229924,\n",
       " 1.1483515348732916,\n",
       " 1.154471435674339,\n",
       " 1.1605942603567578,\n",
       " 1.1667199826143144,\n",
       " 1.1728485766891323,\n",
       " 1.1789800173573737,\n",
       " 1.1851142799153687,\n",
       " 1.191251340166178,\n",
       " 1.1973911744065708,\n",
       " 1.203533759414405,\n",
       " 1.2096790724363962,\n",
       " 1.2158270911762568,\n",
       " 1.2219777937831986,\n",
       " 1.2281311588407797,\n",
       " 1.234287165356089,\n",
       " 1.2404457927492516,\n",
       " 1.2466070208432491,\n",
       " 1.2527708298540383,\n",
       " 1.2589372003809614,\n",
       " 1.2651061133974404,\n",
       " 1.2712775502419373,\n",
       " 1.2774514926091807,\n",
       " 1.2836279225416432,\n",
       " 1.2898068224212644,\n",
       " 1.295988174961411,\n",
       " 1.3021719631990638,\n",
       " 1.3083581704872278,\n",
       " 1.3145467804875532,\n",
       " 1.3207377771631674,\n",
       " 1.3269311447717023,\n",
       " 1.3331268678585182,\n",
       " 1.3393249312501136,\n",
       " 1.3455253200477177,\n",
       " 1.351728019621055,\n",
       " 1.3579330156022835,\n",
       " 1.364140293880095,\n",
       " 1.3703498405939747,\n",
       " 1.3765616421286166,\n",
       " 1.3827756851084863,\n",
       " 1.3889919563925284,\n",
       " 1.3952104430690178,\n",
       " 1.4014311324505413,\n",
       " 1.4076540120691152,\n",
       " 1.413879069671428,\n",
       " 1.42010629321421,\n",
       " 1.4263356708597177,\n",
       " 1.4325671909713409,\n",
       " 1.4388008421093184,\n",
       " 1.4450366130265635,\n",
       " 1.4512744926645977,\n",
       " 1.4575144701495866,\n",
       " 1.4637565347884756,\n",
       " 1.4700006760652211,\n",
       " 1.4762468836371212,\n",
       " 1.4824951473312313,\n",
       " 1.4887454571408725,\n",
       " 1.4949978032222264,\n",
       " 1.501252175891012,\n",
       " 1.5075085656192446,\n",
       " 1.5137669630320738,\n",
       " 1.5200273589047004,\n",
       " 1.5262897441593626,\n",
       " 1.5325541098624014,\n",
       " 1.5388204472213902,\n",
       " 1.5450887475823394,\n",
       " 1.5513590024269617,\n",
       " 1.557631203370005,\n",
       " 1.563905342156649,\n",
       " 1.5701814106599605,\n",
       " 1.5764594008784123,\n",
       " 1.582739304933454,\n",
       " 1.5890211150671445,\n",
       " 1.5953048236398397,\n",
       " 1.601590423127926,\n",
       " 1.607877906121618,\n",
       " 1.6141672653227923,\n",
       " 1.620458493542885,\n",
       " 1.626751583700824,\n",
       " 1.6330465288210196,\n",
       " 1.6393433220313909,\n",
       " 1.6456419565614417,\n",
       " 1.651942425740378,\n",
       " 1.6582447229952682,\n",
       " 1.6645488418492416,\n",
       " 1.6708547759197305,\n",
       " 1.677162518916749,\n",
       " 1.6834720646412071,\n",
       " 1.6897834069832678,\n",
       " 1.6960965399207328,\n",
       " 1.7024114575174685,\n",
       " 1.7087281539218644,\n",
       " 1.7150466233653234,\n",
       " 1.7213668601607872,\n",
       " 1.7276888587012889,\n",
       " 1.734012613458542,\n",
       " 1.7403381189815543,\n",
       " 1.7466653698952754,\n",
       " 1.752994360899267,\n",
       " 1.7593250867664094,\n",
       " 1.7656575423416248,\n",
       " 1.7719917225406392,\n",
       " 1.7783276223487556,\n",
       " 1.7846652368196687,\n",
       " 1.7910045610742884,\n",
       " 1.7973455902996018,\n",
       " 1.8036883197475444,\n",
       " 1.8100327447339082,\n",
       " 1.8163788606372604,\n",
       " 1.8227266628978913,\n",
       " 1.8290761470167816,\n",
       " 1.835427308554588,\n",
       " 1.841780143130651,\n",
       " 1.8481346464220254,\n",
       " 1.854490814162524,\n",
       " 1.8608486421417862,\n",
       " 1.8672081262043603,\n",
       " 1.8735692622488076,\n",
       " 1.8799320462268219,\n",
       " 1.8862964741423656,\n",
       " 1.8926625420508272,\n",
       " 1.8990302460581887,\n",
       " 1.905399582320213,\n",
       " 1.9117705470416477,\n",
       " 1.9181431364754413,\n",
       " 1.9245173469219778,\n",
       " 1.9308931747283213,\n",
       " 1.9372706162874813,\n",
       " 1.9436496680376858,\n",
       " 1.9500303264616723,\n",
       " 1.9564125880859902,\n",
       " 1.9627964494803183,\n",
       " 1.9691819072567915,\n",
       " 1.9755689580693439,\n",
       " 1.981957598613062,\n",
       " 1.9883478256235505,\n",
       " 1.99473963587631,\n",
       " 2.0011330261861278,\n",
       " 2.0075279934064727,\n",
       " 2.013924534428914,\n",
       " 2.020322646182537,\n",
       " 2.026722325633379,\n",
       " 2.033123569783872,\n",
       " 2.0395263756722937,\n",
       " 2.045930740372233,\n",
       " 2.052336660992062,\n",
       " 2.0587441346744155,\n",
       " 2.0651531585956864,\n",
       " 2.071563729965523,\n",
       " 2.0779758460263387,\n",
       " 2.0843895040528317,\n",
       " 2.09080470135151,\n",
       " 2.0972214352602263,\n",
       " 2.103639703147723,\n",
       " 2.110059502413179,\n",
       " 2.116480830485774,\n",
       " 2.1229036848242506,\n",
       " 2.1293280629164903,\n",
       " 2.135753962279096,\n",
       " 2.14218138045698,\n",
       " 2.1486103150229563,\n",
       " 2.1550407635773507,\n",
       " 2.1614727237476026,\n",
       " 2.167906193187887,\n",
       " 2.174341169578734,\n",
       " 2.180777650626658,\n",
       " 2.1872156340637954,\n",
       " 2.1936551176475447,\n",
       " 2.2000960991602136,\n",
       " 2.206538576408674,\n",
       " 2.212982547224018,\n",
       " 2.219428009461228,\n",
       " 2.225874960998843,\n",
       " 2.232323399738636,\n",
       " 2.238773323605297,\n",
       " 2.2452247305461164,\n",
       " 2.251677618530678,\n",
       " 2.25813198555056,\n",
       " 2.264587829619029,\n",
       " 2.271045148770753,\n",
       " 2.27750394106151,\n",
       " 2.283964204567904,\n",
       " 2.2904259373870883,\n",
       " 2.2968891376364873,\n",
       " 2.3033538034535286,\n",
       " 2.309819932995375,\n",
       " 2.3162875244386636,\n",
       " 2.3227565759792506,\n",
       " 2.3292270858319526,\n",
       " 2.335699052230302,\n",
       " 2.342172473426298,\n",
       " 2.3486473476901684,\n",
       " 2.3551236733101275,\n",
       " 2.361601448592146,\n",
       " 2.368080671859717,\n",
       " 2.3745613414536333,\n",
       " 2.3810434557317595,\n",
       " 2.3875270130688153,\n",
       " 2.394012011856157,\n",
       " 2.400498450501567,\n",
       " 2.406986327429041,\n",
       " 2.413475641078583,\n",
       " 2.4199663899060035,\n",
       " 2.4264585723827143,\n",
       " 2.4329521869955357,\n",
       " 2.4394472322465,\n",
       " 2.4459437066526633,\n",
       " 2.4524416087459113,\n",
       " 2.458940937072779,\n",
       " 2.465441690194266,\n",
       " 2.4719438666856544,\n",
       " 2.478447465136337,\n",
       " 2.484952484149636,\n",
       " 2.491458922342635,\n",
       " 2.497966778346008,\n",
       " 2.5044760508038526,\n",
       " 2.5109867383735267,\n",
       " 2.5174988397254854,\n",
       " 2.524012353543118,\n",
       " 2.5305272785225967,\n",
       " 2.537043613372717,\n",
       " 2.5435613568147475,\n",
       " 2.550080507582277,\n",
       " 2.5566010644210673,\n",
       " 2.563123026088906,\n",
       " 2.569646391355466,\n",
       " 2.576171159002156,\n",
       " 2.582697327821989,\n",
       " 2.5892248966194384,\n",
       " 2.5957538642103035,\n",
       " 2.6022842294215773,\n",
       " 2.6088159910913102,\n",
       " 2.6153491480684843,\n",
       " 2.6218836992128813,\n",
       " 2.6284196433949583,\n",
       " 2.6349569794957217,\n",
       " 2.6414957064066042,\n",
       " 2.6480358230293426,\n",
       " 2.65457732827586,\n",
       " 2.6611202210681477,\n",
       " 2.6676645003381463,\n",
       " 2.674210165027633,\n",
       " 2.680757214088109,\n",
       " 2.6873056464806844,\n",
       " 2.6938554611759753,\n",
       " 2.7004066571539855,\n",
       " 2.706959233404008,\n",
       " 2.7135131889245154,\n",
       " 2.720068522723055,\n",
       " 2.7266252338161494,\n",
       " 2.733183321229191,\n",
       " 2.7397427839963484,\n",
       " 2.746303621160458,\n",
       " 2.752865831772938,\n",
       " 2.7594294148936833,\n",
       " 2.765994369590976,\n",
       " 2.7725606949413906,\n",
       " 2.779128390029701,\n",
       " 2.785697453948792,\n",
       " 2.792267885799567,\n",
       " 2.7988396846908596,\n",
       " 2.80541284973935,\n",
       " 2.8119873800694726,\n",
       " 2.8185632748133367,\n",
       " 2.825140533110641,\n",
       " 2.831719154108589,\n",
       " 2.8382991369618074,\n",
       " 2.844880480832268,\n",
       " 2.8514631848892087,\n",
       " 2.858047248309048,\n",
       " 2.864632670275316,\n",
       " 2.871219449978574,\n",
       " 2.8778075866163375,\n",
       " 2.8843970793930054,\n",
       " 2.8909879275197827,\n",
       " 2.8975801302146107,\n",
       " 2.9041736867020953,\n",
       " 2.9107685962134315,\n",
       " 2.9173648579863394,\n",
       " 2.923962471264994,\n",
       " 2.930561435299954,\n",
       " 2.9371617493480966,\n",
       " 2.9437634126725527,\n",
       " 2.9503664245426378,\n",
       " 2.9569707842337896,\n",
       " 2.9635764910275046,\n",
       " 2.9701835442112734,\n",
       " 2.976791943078518,\n",
       " 2.9834016869285325,\n",
       " 2.9900127750664196,\n",
       " 2.9966252068030346,\n",
       " 3.0032389814549205,\n",
       " 3.0098540983442557,\n",
       " 3.0164705567987893,\n",
       " 3.0230883561517907,\n",
       " 3.029707495741988,\n",
       " 3.0363279749135166,\n",
       " 3.0429497930158607,\n",
       " 3.049572949403801,\n",
       " 3.0561974434373584,\n",
       " 3.0628232744817465,\n",
       " 3.0694504419073123,\n",
       " 3.0760789450894883,\n",
       " 3.0827087834087425,\n",
       " 3.0893399562505257,\n",
       " 3.0959724630052214,\n",
       " 3.102606303068096,\n",
       " 3.109241475839256,\n",
       " 3.115877980723589,\n",
       " 3.1225158171307257,\n",
       " 3.1291549844749924,\n",
       " 3.1357954821753555,\n",
       " 3.1424373096553873,\n",
       " 3.1490804663432126,\n",
       " 3.1557249516714654,\n",
       " 3.1623707650772515,\n",
       " 3.1690179060020927,\n",
       " 3.175666373891893,\n",
       " 3.182316168196894,\n",
       " 3.1889672883716282,\n",
       " 3.195619733874884,\n",
       " 3.2022735041696606,\n",
       " 3.2089285987231264,\n",
       " 3.215585017006581,\n",
       " 3.222242758495414,\n",
       " 3.2289018226690676,\n",
       " 3.235562209010995,\n",
       " 3.242223917008623,\n",
       " 3.2488869461533167,\n",
       " 3.2555512959403363,\n",
       " 3.2622169658688054,\n",
       " 3.268883955441671,\n",
       " 3.2755522641656696,\n",
       " 3.2822218915512873,\n",
       " 3.2888928371127304,\n",
       " 3.2955651003678845,\n",
       " 3.3022386808382844,\n",
       " 3.3089135780490757,\n",
       " 3.315589791528984,\n",
       " 3.3222673208102798,\n",
       " 3.3289461654287473,\n",
       " 3.335626324923649,\n",
       " 3.342307798837696,\n",
       " 3.3489905867170116,\n",
       " 3.3556746881111064,\n",
       " 3.3623601025728402,\n",
       " 3.3690468296583957,\n",
       " 3.3757348689272453,\n",
       " 3.3824242199421226,\n",
       " 3.3891148822689905,\n",
       " 3.3958068554770127,\n",
       " 3.4025001391385254,\n",
       " 3.409194732829007,\n",
       " 3.415890636127048,\n",
       " 3.4225878486143273,\n",
       " 3.429286369875578,\n",
       " 3.4359861994985654,\n",
       " 3.442687337074057,\n",
       " 3.4493897821957935,\n",
       " 3.4560935344604653,\n",
       " 3.4627985934676846,\n",
       " 3.469504958819959,\n",
       " 3.476212630122664,\n",
       " 3.4829216069840228,\n",
       " 3.489631889015074,\n",
       " 3.4963434758296486,\n",
       " 3.50305636704435,\n",
       " 3.5097705622785216,\n",
       " 3.5164860611542275,\n",
       " 3.5232028632962282,\n",
       " 3.529920968331956,\n",
       " 3.536640375891487,\n",
       " 3.5433610856075277,\n",
       " 3.550083097115383,\n",
       " 3.5568064100529346,\n",
       " 3.563531024060626,\n",
       " 3.5702569387814296,\n",
       " 3.5769841538608316,\n",
       " 3.5837126689468057,\n",
       " 3.5904424836897957,\n",
       " 3.5971735977426924,\n",
       " 3.603906010760811,\n",
       " 3.610639722401872,\n",
       " 3.6173747323259757,\n",
       " 3.624111040195592,\n",
       " 3.630848645675527,\n",
       " 3.6375875484329137,\n",
       " 3.644327748137186,\n",
       " 3.651069244460061,\n",
       " 3.657812037075521,\n",
       " 3.6645561256597885,\n",
       " 3.6713015098913124,\n",
       " 3.6780481894507497,\n",
       " 3.6847961640209426,\n",
       " 3.691545433286901,\n",
       " 3.6982959969357885,\n",
       " 3.705047854656898,\n",
       " 3.7118010061416364,\n",
       " 3.7185554510835104,\n",
       " 3.7253111891781012,\n",
       " 3.7320682201230535,\n",
       " 3.7388265436180546,\n",
       " 3.7455861593648203,\n",
       " 3.7523470670670758,\n",
       " 3.759109266430538,\n",
       " 3.765872757162903,\n",
       " 3.7726375389738247,\n",
       " 3.7794036115749012,\n",
       " 3.786170974679662,\n",
       " 3.792939628003542,\n",
       " 3.7997095712638775,\n",
       " 3.8064808041798828,\n",
       " 3.8132533264726365,\n",
       " 3.8200271378650688,\n",
       " 3.826802238081943,\n",
       " 3.833578626849842,\n",
       " 3.8403563038971544,\n",
       " 3.8471352689540557,\n",
       " 3.8539155217524996,\n",
       " 3.8606970620261984,\n",
       " 3.867479889510613,\n",
       " 3.8742640039429332,\n",
       " 3.8810494050620714,\n",
       " 3.88783609260864,\n",
       " 3.894624066324945,\n",
       " 3.901413325954967,\n",
       " 3.908203871244353,\n",
       " 3.9149957019403976,\n",
       " 3.921788817792033,\n",
       " 3.9285832185498144,\n",
       " 3.9353789039659093,\n",
       " 3.9421758737940813,\n",
       " 3.948974127789679,\n",
       " 3.955773665709627,\n",
       " 3.9625744873124065,\n",
       " 3.9693765923580457,\n",
       " 3.97617998060811,\n",
       " 3.9829846518256873,\n",
       " 3.9897906057753785,\n",
       " 3.9965978422232804,\n",
       " 4.003406360936981,\n",
       " 4.0102161616855385,\n",
       " 4.0170272442394825,\n",
       " 4.023839608370789,\n",
       " 4.030653253852878,\n",
       " 4.037468180460602,\n",
       " 4.044284387970224,\n",
       " 4.051101876159427,\n",
       " 4.057920644807281,\n",
       " 4.064740693694245,\n",
       " 4.071562022602155,\n",
       " 4.078384631314208,\n",
       " 4.08520851961496,\n",
       " 4.092033687290307,\n",
       " 4.098860134127477,\n",
       " 4.105687859915025,\n",
       " 4.112516864442817,\n",
       " 4.119347147502021,\n",
       " 4.126178708885097,\n",
       " 4.133011548385791,\n",
       " 4.139845665799118,\n",
       " 4.14668106092136,\n",
       " 4.153517733550049,\n",
       " 4.160355683483966,\n",
       " 4.167194910523122,\n",
       " 4.174035414468755,\n",
       " 4.180877195123321,\n",
       " 4.187720252290478,\n",
       " 4.194564585775087,\n",
       " 4.201410195383193,\n",
       " 4.208257080922027,\n",
       " 4.215105242199984,\n",
       " 4.2219546790266245,\n",
       " 4.228805391212662,\n",
       " 4.235657378569954,\n",
       " 4.242510640911494,\n",
       " 4.249365178051406,\n",
       " 4.2562209898049295,\n",
       " 4.263078075988417,\n",
       " 4.269936436419322,\n",
       " 4.276796070916196,\n",
       " 4.283656979298669,\n",
       " 4.2905191613874605,\n",
       " 4.297382617004351,\n",
       " 4.304247345972189,\n",
       " 4.311113348114873,\n",
       " 4.317980623257352,\n",
       " 4.324849171225613,\n",
       " 4.331718991846672,\n",
       " 4.338590084948574,\n",
       " 4.345462450360377,\n",
       " 4.352336087912148,\n",
       " 4.359210997434955,\n",
       " 4.366087178760863,\n",
       " 4.3729646317229225,\n",
       " 4.379843356155162,\n",
       " 4.386723351892587,\n",
       " 4.393604618771166,\n",
       " 4.4004871566278245,\n",
       " 4.407370965300443,\n",
       " 4.414256044627846,\n",
       " 4.421142394449797,\n",
       " 4.4280300146069855,\n",
       " 4.434918904941033,\n",
       " 4.441809065294475,\n",
       " 4.44870049551076,\n",
       " 4.455593195434238,\n",
       " 4.4624871649101605,\n",
       " 4.469382403784672,\n",
       " 4.4762789119047985,\n",
       " 4.48317668911845,\n",
       " 4.490075735274404,\n",
       " 4.49697605022231,\n",
       " 4.503877633812674,\n",
       " 4.510780485896862,\n",
       " 4.517684606327083,\n",
       " 4.524589994956389,\n",
       " 4.5314966516386725,\n",
       " 4.538404576228653,\n",
       " 4.5453137685818765,\n",
       " 4.552224228554707,\n",
       " 4.559135956004323,\n",
       " 4.566048950788711,\n",
       " 4.572963212766654,\n",
       " 4.579878741797739,\n",
       " 4.586795537742338,\n",
       " 4.593713600461611,\n",
       " 4.600632929817497,\n",
       " 4.60755352567271,\n",
       " 4.61447538789073,\n",
       " 4.621398516335804,\n",
       " 4.628322910872934,\n",
       " 4.63524857136788,\n",
       " 4.642175497687143,\n",
       " 4.649103689697972,\n",
       " 4.656033147268352,\n",
       " 4.662963870266998,\n",
       " 4.669895858563358,\n",
       " 4.676829112027594,\n",
       " 4.683763630530593,\n",
       " 4.6906994139439515,\n",
       " 4.6976364621399735,\n",
       " 4.7045747749916655,\n",
       " 4.711514352372731,\n",
       " 4.71845519415757,\n",
       " 4.725397300221268,\n",
       " 4.732340670439595,\n",
       " 4.739285304689001,\n",
       " 4.7462312028466105,\n",
       " 4.753178364790215,\n",
       " 4.760126790398275,\n",
       " 4.767076479549909,\n",
       " 4.774027432124898,\n",
       " 4.780979648003666,\n",
       " 4.78793312706729,\n",
       " 4.794887869197488,\n",
       " 4.801843874276622,\n",
       " 4.808801142187681,\n",
       " 4.815759672814289,\n",
       " 4.822719466040696,\n",
       " 4.829680521751773,\n",
       " 4.83664283983301,\n",
       " 4.843606420170506,\n",
       " 4.850571262650977,\n",
       " 4.85753736716174,\n",
       " 4.864504733590711,\n",
       " 4.8714733618264106,\n",
       " 4.878443251757946,\n",
       " 4.8854144032750195,\n",
       " 4.892386816267914,\n",
       " 4.8993604906274975,\n",
       " 4.906335426245216,\n",
       " 4.913311623013089,\n",
       " 4.920289080823704,\n",
       " 4.9272677995702185,\n",
       " 4.934247779146349,\n",
       " 4.941229019446379,\n",
       " 4.948211520365137,\n",
       " 4.955195281798013,\n",
       " 4.962180303640938,\n",
       " 4.969166585790392,\n",
       " 4.976154128143392,\n",
       " 4.983142930597497,\n",
       " 4.990132993050799,\n",
       " 4.997124315401919,\n",
       " 5.004116897550007,\n",
       " 5.011110739394733,\n",
       " 5.018105840836294,\n",
       " 5.025102201775398,\n",
       " 5.032099822113268,\n",
       " 5.039098701751638,\n",
       " 5.046098840592752,\n",
       " 5.0531002385393515,\n",
       " 5.060102895494682,\n",
       " 5.0671068113624855,\n",
       " 5.074111986047002,\n",
       " 5.081118419452955,\n",
       " 5.088126111485562,\n",
       " 5.095135062050522,\n",
       " 5.102145271054017,\n",
       " 5.109156738402705,\n",
       " 5.116169464003723,\n",
       " 5.123183447764677,\n",
       " 5.130198689593645,\n",
       " 5.137215189399172,\n",
       " 5.144232947090262,\n",
       " 5.1512519625763815,\n",
       " 5.158272235767458,\n",
       " 5.165293766573869,\n",
       " 5.1723165549064465,\n",
       " 5.179340600676468,\n",
       " 5.186365903795663,\n",
       " 5.193392464176199,\n",
       " 5.200420281730687,\n",
       " 5.207449356372172,\n",
       " 5.21447968801414,\n",
       " 5.221511276570502,\n",
       " 5.228544121955605,\n",
       " 5.235578224084218,\n",
       " 5.242613582871538,\n",
       " 5.249650198233179,\n",
       " 5.256688070085177,\n",
       " 5.263727198343988,\n",
       " 5.270767582926471,\n",
       " 5.277809223749904,\n",
       " 5.284852120731975,\n",
       " 5.2918962737907735,\n",
       " 5.298941682844792,\n",
       " 5.305988347812927,\n",
       " 5.313036268614473,\n",
       " 5.320085445169118,\n",
       " 5.327135877396947,\n",
       " 5.334187565218433,\n",
       " 5.34124050855444,\n",
       " 5.348294707326218,\n",
       " 5.3553501614554,\n",
       " 5.362406870863999,\n",
       " 5.369464835474412,\n",
       " 5.3765240552094085,\n",
       " 5.3835845299921345,\n",
       " 5.390646259746109,\n",
       " 5.3977092443952195,\n",
       " 5.404773483863721,\n",
       " 5.411838978076236,\n",
       " 5.41890572695775,\n",
       " 5.425973730433605,\n",
       " 5.43304298842951,\n",
       " 5.440113500871524,\n",
       " 5.447185267686063,\n",
       " 5.454258288799896,\n",
       " 5.461332564140141,\n",
       " 5.468408093634265,\n",
       " 5.475484877210081,\n",
       " 5.4825629147957455,\n",
       " 5.489642206319756,\n",
       " 5.496722751710952,\n",
       " 5.503804550898509,\n",
       " 5.510887603811938,\n",
       " 5.517971910381085,\n",
       " 5.525057470536129,\n",
       " 5.532144284207574,\n",
       " 5.539232351326256,\n",
       " 5.546321671823336,\n",
       " 5.553412245630298,\n",
       " 5.560504072678947,\n",
       " 5.56759715290141,\n",
       " 5.574691486230132,\n",
       " 5.581787072597871,\n",
       " 5.588883911937703,\n",
       " 5.595982004183015,\n",
       " 5.603081349267505,\n",
       " 5.610181947125178,\n",
       " 5.61728379769035,\n",
       " 5.624386900897635,\n",
       " 5.631491256681957,\n",
       " 5.638596864978539,\n",
       " 5.645703725722904,\n",
       " 5.652811838850873,\n",
       " 5.659921204298559,\n",
       " 5.667031822002378,\n",
       " 5.674143691899033,\n",
       " 5.6812568139255175,\n",
       " 5.688371188019114,\n",
       " 5.695486814117399,\n",
       " 5.702603692158227,\n",
       " 5.709721822079738,\n",
       " 5.716841203820362,\n",
       " 5.723961837318799,\n",
       " 5.731083722514036,\n",
       " 5.738206859345334,\n",
       " 5.745331247752234,\n",
       " 5.752456887674547,\n",
       " 5.759583779052356,\n",
       " 5.766711921826022,\n",
       " 5.773841315936169,\n",
       " 5.780971961323692,\n",
       " 5.788103857929749,\n",
       " 5.795237005695771,\n",
       " 5.802371404563445,\n",
       " 5.809507054474722,\n",
       " 5.8166439553718154,\n",
       " 5.823782107197194,\n",
       " 5.830921509893587,\n",
       " 5.838062163403979,\n",
       " 5.8452040676716095,\n",
       " 5.852347222639969,\n",
       " 5.8594916282527985,\n",
       " 5.866637284454097,\n",
       " 5.873784191188102,\n",
       " 5.880932348399305,\n",
       " 5.888081756032441,\n",
       " 5.895232414032489,\n",
       " 5.902384322344672,\n",
       " 5.909537480914456,\n",
       " 5.916691889687544,\n",
       " 5.923847548609881,\n",
       " 5.931004457627649,\n",
       " 5.938162616687265,\n",
       " 5.945322025735381,\n",
       " 5.952482684718881,\n",
       " 5.959644593584887,\n",
       " 5.966807752280747,\n",
       " 5.973972160754041,\n",
       " 5.981137818952574,\n",
       " 5.9883047268243805,\n",
       " 5.995472884317722,\n",
       " 6.002642291381082,\n",
       " 6.009812947963169,\n",
       " 6.016984854012913,\n",
       " 6.0241580094794625,\n",
       " 6.031332414312188,\n",
       " 6.038508068460678,\n",
       " 6.0456849718747385,\n",
       " 6.052863124504391,\n",
       " 6.060042526299869,\n",
       " 6.067223177211623,\n",
       " 6.074405077190313,\n",
       " 6.081588226186812,\n",
       " 6.088772624152204,\n",
       " 6.095958271037777,\n",
       " 6.103145166795032,\n",
       " 6.110333311375671,\n",
       " 6.117522704731605,\n",
       " 6.124713346814951,\n",
       " 6.131905237578022,\n",
       " 6.1390983769733385,\n",
       " 6.146292764953621,\n",
       " 6.153488401471787,\n",
       " 6.160685286480957,\n",
       " 6.167883419934444,\n",
       " 6.175082801785759,\n",
       " 6.182283431988612,\n",
       " 6.189485310496902,\n",
       " 6.196688437264721,\n",
       " 6.20389281224636,\n",
       " 6.21109843539629,\n",
       " 6.218305306669183,\n",
       " 6.225513426019895,\n",
       " 6.232722793403467,\n",
       " 6.239933408775133,\n",
       " 6.2471452720903065,\n",
       " 6.25435838330459,\n",
       " 6.261572742373769,\n",
       " 6.268788349253812,\n",
       " 6.276005203900867,\n",
       " 6.2832233062712675,\n",
       " 6.290442656321521,\n",
       " 6.297663254008319,\n",
       " 6.304885099288526,\n",
       " 6.31210819211919,\n",
       " 6.319332532457529,\n",
       " 6.3265581202609384,\n",
       " 6.333784955486985,\n",
       " 6.341013038093417,\n",
       " 6.348242368038145,\n",
       " 6.355472945279252,\n",
       " 6.362704769775,\n",
       " 6.369937841483812,\n",
       " 6.377172160364282,\n",
       " 6.384407726375172,\n",
       " 6.391644539475409,\n",
       " 6.3988825996240895,\n",
       " 6.406121906780473,\n",
       " 6.413362460903979,\n",
       " 6.420604261954197,\n",
       " 6.4278473098908755,\n",
       " 6.435091604673924,\n",
       " 6.442337146263414,\n",
       " 6.449583934619575,\n",
       " 6.4568319697027965,\n",
       " 6.464081251473626,\n",
       " 6.471331779892767,\n",
       " 6.478583554921081,\n",
       " 6.485836576519585,\n",
       " 6.493090844649449,\n",
       " 6.500346359271997,\n",
       " 6.50760312034871,\n",
       " 6.514861127841215,\n",
       " 6.522120381711295,\n",
       " 6.529380881920883,\n",
       " 6.536642628432059,\n",
       " 6.543905621207056,\n",
       " 6.55116986020825,\n",
       " 6.5584353453981725,\n",
       " 6.565702076739495,\n",
       " 6.572970054195037,\n",
       " 6.580239277727762,\n",
       " 6.587509747300781,\n",
       " 6.594781462877345,\n",
       " 6.602054424420849,\n",
       " 6.609328631894834,\n",
       " 6.616604085262977,\n",
       " 6.6238807844890975,\n",
       " 6.631158729537156,\n",
       " 6.638437920371253,\n",
       " 6.645718356955621,\n",
       " 6.65300003925464]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9305555555555556"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different learning rates and compare the results. How does the learning rate influence the convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different regularization parameter values and compare the model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Compare zero initialization and random initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you need to implement weighted K-Neighbors Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that training a KNN classifier is simply memorizing a training sample. \n",
    "\n",
    "The process of applying a classifier for one object is to find the distances from it to all objects in the training data, then select the k nearest objects (neighbors) and return the most common class among these objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also give the nearest neighbors weights in accordance with the distance of the object to them. In the simplest case (as in your assignment), you can set the weights inversely proportional to that distance. \n",
    "\n",
    "$$w_{i} = \\frac{1}{d_{i} + eps},$$\n",
    "\n",
    "where $d_{i}$ is the distance between object and i-th nearest neighbor and $eps$ is the small value to prevent division by zero.\n",
    "\n",
    "In case of 'uniform' weights, all k nearest neighbors are equivalent (have equal weight, for example $w_{i} = 1, \\forall i \\in(1,k)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the probability of classes, it is necessary to normalize the weights of each class, dividing them by the sum:\n",
    "\n",
    "$$p_{i} = \\frac{w_{i}}{\\sum_{j=1}^{c}w_{j}},$$\n",
    "\n",
    "where $p_i$ is probability of i-th class and $c$ is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2 points)** Implement the algorithm and use it to classify the digits. By implementing this algorithm, you will be able to classify numbers not only into \"even\" or \"odd\", but into their real representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKNeighborsClassifier:\n",
    "    _estimator_type = \"classifier\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5, weights='uniform', eps=1e-9):\n",
    "        \"\"\"K-Nearest Neighbors classifier.\n",
    "        \n",
    "        Args:\n",
    "            n_neighbors: int, default=5\n",
    "                Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
    "            weights : {'uniform', 'distance'} or callable, default='uniform'\n",
    "                Weight function used in prediction.  Possible values:\n",
    "                - 'uniform' : uniform weights.  All points in each neighborhood\n",
    "                  are weighted equally.\n",
    "                - 'distance' : weight points by the inverse of their distance.\n",
    "                  in this case, closer neighbors of a query point will have a\n",
    "                  greater influence than neighbors which are further away.\n",
    "            eps : float, default=1e-5\n",
    "                Epsilon to prevent division by 0 \n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.eps = eps\n",
    "        \n",
    "    \n",
    "    def get_pairwise_distances(self, X, Y):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returnes matrix of the pairwise distances between the rows from both X and Y.\n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            Y: numpy array of shape (k_samples, n_features)\n",
    "        Returns:\n",
    "            P: numpy array of shape (n_samples, k_samples)\n",
    "                Matrix in which (i, j) value is the distance \n",
    "                between i'th row from the X and j'th row from the Y.\n",
    "        \"\"\"\n",
    "        return np.array([[sqrt(sum((i-j)**2)) for j in Y] for i in X])\n",
    "    \n",
    "    \n",
    "    def get_class_weights(self, y, weights):\n",
    "        \"\"\"\n",
    "        Returns a vector with sum of weights for each class \n",
    "        Args:\n",
    "            y: numpy array of shape (n_samles,)\n",
    "            weights: numpy array of shape (n_samples,)\n",
    "                The weights of the corresponding points of y.\n",
    "        Returns:\n",
    "            p: numpy array of shape (n_classes)\n",
    "                Array where the value at the i-th position \n",
    "                corresponds to the weight of the i-th class.\n",
    "        \"\"\"\n",
    "        # <your code>\n",
    "        pass\n",
    "            \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Target vector.        \n",
    "        \"\"\"\n",
    "        self.points = X\n",
    "        self.y = y\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict positive class probabilities.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples, n_classes)\n",
    "                Vector containing positive class probabilities.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'points'):\n",
    "            P = self.get_pairwise_distances(X, self.points)\n",
    "            \n",
    "            weights_of_points = np.ones(P.shape)\n",
    "            if self.weights == 'distance':\n",
    "                weights_of_points = 'your code'\n",
    "                \n",
    "            # <your code>\n",
    "            pass\n",
    "        \n",
    "        else: \n",
    "            raise NotFittedError(\"CustomKNeighborsClassifier instance is not fitted yet\")\n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing predicted class labels.\n",
    "        \"\"\"\n",
    "        # <your code>\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomKNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(model.get_pairwise_distances(np.array([[0  , 1]  , [1, 1]]), \n",
    "                                                np.array([[0.5, 0.5], [1, 0]])),\n",
    "                   np.array([[0.70710678, 1.41421356],\n",
    "                             [0.70710678, 1.        ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_ = ['one', 'two', 'three']\n",
    "assert np.allclose(model.get_class_weights(np.array(['one', 'one', 'three', 'two']), np.array([1, 1, 0, 4])), \n",
    "                   np.array([2,4,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "_, axes = plt.subplots(nrows=3, ncols=7, figsize=(10, 5))\n",
    "for ax, image, label in zip(axes.flatten(), X, y):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.gray_r if label % 2 else plt.cm.afmhot_r)\n",
    "    ax.set_title(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "knn.fit(X_train, list(map(str, y_train)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(model.predict_proba(X_test), knn.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, test_acc = fit_evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_acc == 1\n",
    "assert test_acc > 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Take a look at the confusion matrix and tell what numbers the model confuses and why this happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different n_neighbors parameters and compare the output probabilities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Compare both 'uniform' and 'distance' weights and share your thoughts in what situations which parameter can be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Suggest another distance measurement function that could improve the quality of the classification for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Suggest different task and distance function that you think would be suitable for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Synthetic Titanic Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Read the description here: https://www.kaggle.com/c/tabular-playground-series-apr-2021/data. Download the dataset and place it in the *data/titanic/* folder in your working directory.\n",
    "You will use train.csv for model training and validation. The test set is used for model testing: once the model is trained, you can predict whether a passenger survived or not for each passenger in the test set, and submit the predictions: https://www.kaggle.com/c/tabular-playground-series-apr-2021/overview/evaluation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(PATH, 'train.csv')).set_index('PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** How many females and males are there in the dataset? What about the survived passengers? Is there any relationship between the gender and the survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(data['Sex'].value_counts())\n",
    "sns.countplot(x=\"Sex\", data=data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=data, x='Sex', hue='Survived');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**       : 56114  43886.         ,     **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Plot age distribution of the passengers. What is the average and the median age of survived and deceased passengers? Do age distributions differ for survived and deceased passengers? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point)** Explore \"passenger class\" and \"embarked\" features. What class was \"the safest\"? Is there any relationship between the embarkation port and the survival? Provide the corresponding visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Find the percentage of missing values for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the ways to handle these missing values for modelling and write your answer below. Which methods would you suggest? What are their advantages and disadvantages?\n",
    "\n",
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.5 points)** Prepare the features and train two models (KNN and Logistic Regression) to predict the survival. Compare the results. Use accuracy as a metric. Don't forget about cross-validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 + X points)** Try more feature engineering and hyperparameter tuning to improve the results. You may use either KNN or Logistic Regression (or both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best model, load the test set and make the predictions. Submit them to kaggle and see the results :)\n",
    "\n",
    "**Note**. X points will depend on your kaggle public leaderboard score.\n",
    "$$ f(score) = 1.0, \\ \\ 0.79 \\leq score < 0.80,$$\n",
    "$$ f(score) = 2.5, \\ \\ 0.80 \\leq score < 0.81,$$ \n",
    "$$ f(score) = 4.0, \\ \\ 0.81 \\leq score $$ \n",
    "Your code should generate the output submitted to kaggle. Fix random seeds to make the results reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
